# -*- coding: utf-8 -*-
"""Emotion_Detection_from_tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jzxYGxqhe_s5aRlcSA4NdOxdfEkTjSih

Installation of Libraries
"""

#!pip install numpy
#!pip install pandas
#!pip install seaborn
#!pip install nltk
#!pip install matplotlib
#!pip install wordcloud
#!pip install scikit-learn
#!pip install tensorflow

"""Loading the Datasets and Printing of dataset"""

# Import necessary libraries
import warnings
warnings.filterwarnings('ignore')
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import seaborn as sns
import re
import nltk
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Load the dataset into a pandas DataFrame
df = pd.read_csv('emotion_text.csv')

# Display the first few rows of the dataset
print(df.head())

# Proceed with your analysis and machine learning tasks

"""Train, Test and Validation Dataset"""

# Modified load_dataset function using pandas
def load_dataset(filepath):
    # Using pandas to read the file with ';' as the delimiter
    return pd.read_csv(filepath, delimiter=';', header=None, names=["text", "emotion"])

# Example usage:
train_data = load_dataset("/content/train.txt")
validation_data = load_dataset("/content/val.txt")
test_data = load_dataset("/content/test.txt")

# Display the first few rows of each dataset
print("Training Data:")
print(train_data.head())

print("\nValidation Data:")
print(validation_data.head())

print("\nTest Data:")
print(test_data.head())

"""Combining Train, Test and Validation dataset"""

df = pd.concat([train_data, validation_data, test_data])
df.head()

df.shape

"""Data Preprocessing"""

# Import necessary libraries for text processing
import nltk
import re
import string

# Download NLTK data resources
nltk.download('stopwords')
nltk.download('wordnet')  # Required for lemmatization
nltk.download('omw-1.4')  # WordNet lemmatizer updates

# Initialize text processing tools
lemmatizer = nltk.stem.WordNetLemmatizer()
stopwords = set(nltk.corpus.stopwords.words("english"))
stemmer = nltk.stem.snowball.SnowballStemmer("english")

# Additional preprocessing functions
def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()

    # 2. Remove special characters and numbers (keep only alphabets and spaces)
    text = re.sub(r'[^a-z\s]', '', text)

    # 3. Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # 4. Tokenize text
    tokens = text.split()

    # 5. Remove stopwords
    tokens = [word for word in tokens if word not in stopwords]

    # 6. Apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # 7. Optionally, apply stemming (uncomment if required)
    # tokens = [stemmer.stem(word) for word in tokens]

    # Join tokens back into a single string
    processed_text = ' '.join(tokens)
    return processed_text

# Example of applying this to a DataFrame (assuming df is your DataFrame with a 'text' column)
df['cleaned_text'] = df['text'].apply(preprocess_text)

# Display the cleaned data
print(df[['text', 'cleaned_text']].head())

"""Exploratory Data Analysis"""

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.info()

df['emotion'].value_counts().plot(kind='bar', color=['green', 'gray', 'pink', 'black', 'red', 'orange'])
plt.xlabel('Emotions')
plt.ylabel('Number of samples')
plt.show()

df['length'] = df.text.apply(lambda x:len(x))
fig = plt.figure(figsize=(10,6))
sns.kdeplot(x=df['length'], hue=df["emotion"])
plt.show()

emotions = df['emotion'].unique()
for emotion in emotions:
    text = " ".join(df[df['emotion'] == emotion]['text'])
    wordcloud = WordCloud(width = 800, height = 800,
                    background_color ='white',
                    stopwords = stopwords,
                    min_font_size = 10).generate(text)
    plt.figure(figsize = (4, 4), facecolor = None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad = 0)
    plt.title(emotion)
    plt.show()

"""Training data and Testing data split"""

X_train, X_test, y_train, y_test = train_test_split(df["text"], df["emotion"], test_size=0.2, random_state=42)

"""**Modeling**"""

cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_test_cv = cv.transform(X_test)

"""**Logistic** **Regression**"""

from sklearn.linear_model import LogisticRegression

# Train Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, random_state=100)
log_reg.fit(X_train_cv, y_train)

# Make predictions
y_pred_log_reg = log_reg.predict(X_test_cv)

# Classification report
report_log_reg = classification_report(y_test, y_pred_log_reg)
print("Classification report of Logistic Regression:\n", report_log_reg)

# Accuracy score
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
print("Accuracy of Logistic Regression:", accuracy_log_reg)

"""**Multinomial** **Naive** **Bayes**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

# Train Multinomial Naive Bayes model
mnb = MultinomialNB()
mnb.fit(X_train_cv, y_train)

# Make predictions
y_pred_mnb = mnb.predict(X_test_cv)

# Classification report
report_mnb = classification_report(y_test, y_pred_mnb)
print("Classification report of Multinomial Naive Bayes:\n", report_mnb)

# Accuracy score
accuracy_mnb = accuracy_score(y_test, y_pred_mnb)
print("Accuracy of Multinomial Naive Bayes:", accuracy_mnb)

"""**LSTM**"""

from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder

# Assuming df is your DataFrame with 'text' and 'emotion' columns
X = df['text']
y = df['emotion']

# Encode the labels (emotions)
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # This will give you the labels as [0, 1, 2, 3, 4, 5]
emotions = le.classes_  # This will give you the original emotion names

# Tokenize and pad the sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
max_len = 100  # Adjust as necessary
X_pad = pad_sequences(X_seq, maxlen=max_len)

# Split the data
from sklearn.model_selection import train_test_split
X_train_pad, X_test_pad, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)

# Define LSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(len(emotions), activation='softmax'))  # Multi-class output layer

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define EarlyStopping callback
early_stop = EarlyStopping(monitor='val_loss', patience=3)

# Train the model
history = model.fit(X_train_pad, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop])

# Evaluate the model on test data
y_pred_probs = model.predict(X_test_pad)
y_pred_lstm = y_pred_probs.argmax(axis=1)  # Get the index of the highest probability

# Classification report for LSTM
report_lstm = classification_report(y_test, y_pred_lstm, target_names=emotions)
print("Classification report of LSTM:\n", report_lstm)

# Accuracy score
accuracy_lstm = accuracy_score(y_test, y_pred_lstm)
print("Accuracy of LSTM:", accuracy_lstm)

"""**Model** **Ensembling**


"""

import numpy as np
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_pred_log_reg and y_pred_mnb are the predictions from the respective models
# Ensure LSTM predictions are converted from probabilities to class labels
y_pred_lstm_classes = (y_pred_lstm > 0.5).astype("int32").flatten()  # Flatten to ensure it's a 1D array

# Convert the predictions from logistic regression and multinomial naive bayes to their integer labels
# Ensure that you have already transformed your labels using LabelEncoder
y_pred_log_reg_int = le.transform(y_pred_log_reg)  # Transform if still in string format
y_pred_mnb_int = le.transform(y_pred_mnb)  # Transform if still in string format

# Stack the predictions to form a 2D array
predictions = np.array([y_pred_log_reg_int, y_pred_mnb_int, y_pred_lstm_classes], dtype=int)  # Ensure int type

# Perform majority voting
# This works by taking the mode across the predictions
ensemble_predictions = np.array([np.bincount(pred).argmax() for pred in predictions.T])

# Evaluate ensemble model
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
print("Accuracy of Ensemble Model: {:.2f}".format(ensemble_accuracy))

# Generate classification report for ensemble model
report_ensemble = classification_report(y_test, ensemble_predictions, target_names=emotions)
print("Classification report of Ensemble Model:\n", report_ensemble)
